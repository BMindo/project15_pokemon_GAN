{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    %tensorflow_version 2.x\nexcept:\n    pass","execution_count":254,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"SETUP = True\nif SETUP:\n    !pip install -q -U toai\n    # !pip install -q -U nb_black\n    # !pip install -q -U tensorflow-datasets\n    !pip install -q  --no-deps tensorflow-addons~=0.6\n    print(__import__(\"toai\").__version__)\n    print(__import__(\"tensorflow\").__version__)","execution_count":255,"outputs":[{"output_type":"stream","text":"0.3.9\n2.1.0\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from toai.imports import *\nfrom toai.data import DataBundle, DataParams, DataContainer\n# from toai.image import ImageParser, ImageScaler\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":256,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import *","execution_count":257,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/pokemon-images-and-types/images/images\")","execution_count":258,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nSHUFFLE_SIZE = 1024\nIMG_DIMS = (64, 64, 3)\nN_IMAGES = len(os.listdir(DATA_DIR))","execution_count":313,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import attr\n\n@attr.s(auto_attribs=True)\nclass ImageParser:\n    n_channels: int = 3\n\n    def __call__(self, filename: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        image = tf.image.decode_jpeg(\n            tf.io.read_file(filename), channels=self.n_channels\n        )\n        image = tf.image.convert_image_dtype(image, tf.float32)\n\n        return image, label","execution_count":314,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@attr.s(auto_attribs=True)\nclass ImageResizer:\n    img_dims: Tuple[int, int, int]\n    resize: Optional[str] = None\n    crop_adjustment: float = 1\n\n    def __call__(self, image: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        height, width, _ = self.img_dims\n        if self.resize == \"stretch\":\n            image = tf.image.resize(image, (height, width))\n        elif self.resize == \"crop\":\n            crop_height, crop_width = [\n                int(x * self.crop_adjustment) for x in (height, width)\n            ]\n            image = tf.image.resize(\n                images=image, size=(crop_height, crop_width), preserve_aspect_ratio=True\n            )\n            image = tf.image.resize_with_crop_or_pad(image, height, width)\n        elif self.resize == \"random_crop\":\n            crop_height, crop_width = [\n                int(x * self.crop_adjustment) for x in (height, width)\n            ]\n            image = tf.image.resize(image, (crop_height, crop_width))\n            image = tf.image.random_crop(image, self.img_dims)\n\n        return image, label","execution_count":315,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@attr.s(auto_attribs=True)\nclass ImageScaler:\n    scale_fn: Callable\n\n    def __call__(self, image: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        return self.scale_fn(image), label","execution_count":316,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef preprocess_input(x):\n    return x / 255.0","execution_count":317,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = (\n    tf.data.Dataset.from_tensor_slices(([str(x) for x in DATA_DIR.glob('**/*')], os.listdir(DATA_DIR)))\n    .map(map_func=ImageParser())\n    .map(map_func=ImageResizer(IMG_DIMS, \"stretch\"))\n    .repeat()\n    .shuffle(SHUFFLE_SIZE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","execution_count":318,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, y in train_data.take(1):\n    print(y[0])\n    print(x[0].numpy().min(), x[0].numpy().max())\n    plt.imshow(x.numpy()[0])\n    ","execution_count":319,"outputs":[{"output_type":"stream","text":"tf.Tensor(b'charjabug.jpg', shape=(), dtype=string)\n0.12380516 1.0\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x396 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"codings_size = 32","execution_count":320,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = keras.models.Sequential(\n    [\n        keras.layers.Dense(8 * 8 * 128, input_shape=[codings_size]),\n        keras.layers.Reshape([8, 8, 128]),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2DTranspose(\n            64,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.activations.selu,\n            kernel_initializer=keras.initializers.lecun_uniform(),\n        ),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2DTranspose(\n            32,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.activations.selu,\n            kernel_initializer=keras.initializers.lecun_uniform(),\n        ),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2DTranspose(\n            IMG_DIMS[2], kernel_size=5, strides=2, padding=\"SAME\", activation=keras.activations.sigmoid\n        ),\n    ]\n)","execution_count":321,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.summary()","execution_count":322,"outputs":[{"output_type":"stream","text":"Model: \"sequential_59\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_37 (Dense)             (None, 8192)              270336    \n_________________________________________________________________\nreshape_14 (Reshape)         (None, 8, 8, 128)         0         \n_________________________________________________________________\nbatch_normalization_42 (Batc (None, 8, 8, 128)         512       \n_________________________________________________________________\nconv2d_transpose_42 (Conv2DT (None, 16, 16, 64)        204864    \n_________________________________________________________________\nbatch_normalization_43 (Batc (None, 16, 16, 64)        256       \n_________________________________________________________________\nconv2d_transpose_43 (Conv2DT (None, 32, 32, 32)        51232     \n_________________________________________________________________\nbatch_normalization_44 (Batc (None, 32, 32, 32)        128       \n_________________________________________________________________\nconv2d_transpose_44 (Conv2DT (None, 64, 64, 3)         2403      \n=================================================================\nTotal params: 529,731\nTrainable params: 529,283\nNon-trainable params: 448\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = keras.models.Sequential(\n    [\n#         keras.layers.Conv2D(\n#             64,\n#             kernel_size=5,\n#             strides=2,\n#             padding=\"SAME\",\n#             activation=keras.layers.LeakyReLU(0.2),\n#             kernel_initializer=keras.initializers.he_uniform(),\n#             input_shape=IMG_DIMS,\n#         ),\n#         keras.layers.Dropout(0.5),\n        keras.layers.Conv2D(\n            128,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n            input_shape=IMG_DIMS,\n        ),\n        keras.layers.Dropout(0.4),\n        keras.layers.Conv2D(\n            256,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n        ),\n        keras.layers.Dropout(0.4),\n        keras.layers.Conv2D(\n            512,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n        ),\n        keras.layers.Dropout(0.4),\n#         keras.layers.Conv2D(\n#             1028,\n#             kernel_size=5,\n#             strides=2,\n#             padding=\"SAME\",\n#             activation=keras.layers.LeakyReLU(0.2),\n#             kernel_initializer=keras.initializers.he_uniform(),\n#         ),\n#         keras.layers.Dropout(0.5),\n        keras.layers.Flatten(),\n        keras.layers.Dense(1, activation=keras.activations.sigmoid),\n    ]\n)","execution_count":323,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# discriminator1 = discriminator","execution_count":324,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# discriminator = keras.models.Sequential([\n#     keras.applications.VGG16(include_top=False, weights=None),\n#     keras.layers.GlobalMaxPool2D(),\n#     keras.layers.Dropout(0.5),\n#     keras.layers.Dense(1, activation=keras.activations.sigmoid),\n# ])","execution_count":325,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator.summary()","execution_count":326,"outputs":[{"output_type":"stream","text":"Model: \"sequential_60\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_69 (Conv2D)           (None, 32, 32, 128)       9728      \n_________________________________________________________________\ndropout_70 (Dropout)         (None, 32, 32, 128)       0         \n_________________________________________________________________\nconv2d_70 (Conv2D)           (None, 16, 16, 256)       819456    \n_________________________________________________________________\ndropout_71 (Dropout)         (None, 16, 16, 256)       0         \n_________________________________________________________________\nconv2d_71 (Conv2D)           (None, 8, 8, 512)         3277312   \n_________________________________________________________________\ndropout_72 (Dropout)         (None, 8, 8, 512)         0         \n_________________________________________________________________\nflatten_22 (Flatten)         (None, 32768)             0         \n_________________________________________________________________\ndense_38 (Dense)             (None, 1)                 32769     \n=================================================================\nTotal params: 4,139,265\nTrainable params: 4,139,265\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    plt.figure(figsize=(n_cols * 2, n_rows * 2))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")","execution_count":327,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_gan(gan, dataset, n_images, batch_size, codings_size, n_epochs=1):\n    generator, discriminator = gan.layers\n    for epoch in range(n_epochs):\n        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n        for train_batch, _ in dataset.take(n_images // batch_size):\n            # phase 1 - training the discriminator\n            noise = tf.random.normal(shape=[batch_size, codings_size])\n            generated_images = generator(noise)\n            fake_and_real_data = tf.concat([generated_images, train_batch], axis=0)\n            fake_and_real_labels = tf.constant([[0.0]] * batch_size + [[1.0]] * batch_size)\n            discriminator.trainable = True\n            discriminator.train_on_batch(fake_and_real_data, fake_and_real_labels)\n            # phase 2 - training the generator\n            noise = tf.random.normal(shape=[batch_size, codings_size])\n            noise_labels = tf.constant([[1.0]] * batch_size)\n            discriminator.trainable = False\n            gan.train_on_batch(noise, noise_labels)\n        plot_multiple_images(generated_images, 8)\n        plt.show()","execution_count":328,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = keras.models.Sequential([generator, discriminator])","execution_count":329,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# discriminator.compile(\n#     loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(3e-4)\n# )\n# discriminator.trainable = False\n# gan.compile(\n#     loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(1e-4)\n# )","execution_count":298,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator.compile(\n    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop(3e-4)\n)\ndiscriminator.trainable = False\ngan.compile(\n    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop(3e-3)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\nSHUFFLE_SIZE = 1024\nIMG_DIMS = (64, 64, 3)\nN_IMAGES = len(os.listdir(DATA_DIR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"codings_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = keras.models.Sequential(\n    [\n        keras.layers.Dense(8 * 8 * 256, input_shape=[codings_size]),\n        keras.layers.Reshape([8, 8, 256]),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2DTranspose(\n            256,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.activations.selu,\n            kernel_initializer=keras.initializers.lecun_uniform(),\n        ),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2DTranspose(\n            128,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.activations.selu,\n            kernel_initializer=keras.initializers.lecun_uniform(),\n        ),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2DTranspose(\n            IMG_DIMS[2], kernel_size=5, strides=2, padding=\"SAME\", activation=keras.activations.sigmoid\n        ),\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = keras.models.Sequential(\n    [\n        keras.layers.Conv2D(\n            128,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n            input_shape=IMG_DIMS,\n        ),\n        keras.layers.Dropout(0.4),\n        keras.layers.Conv2D(\n            256,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n        ),\n        keras.layers.Dropout(0.4),\n        keras.layers.Conv2D(\n            512,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n        ),\n        keras.layers.Dropout(0.4),\n        keras.layers.Conv2D(\n            1024,\n            kernel_size=5,\n            strides=2,\n            padding=\"SAME\",\n            activation=keras.layers.LeakyReLU(0.2),\n            kernel_initializer=keras.initializers.he_uniform(),\n        ),\n        keras.layers.Dropout(0.4),\n        keras.layers.Flatten(),\n        keras.layers.Dense(1, activation=keras.activations.sigmoid),\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# discriminator1 = discriminator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# discriminator = keras.models.Sequential([\n#     keras.applications.VGG16(include_top=False, weights=None),\n#     keras.layers.GlobalMaxPool2D(),\n#     keras.layers.Dropout(0.4),\n#     keras.layers.Dense(1, activation=keras.activations.sigmoid),\n# ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_gan(gan, dataset, n_images, batch_size, codings_size, n_epochs=1):\n    generator, discriminator = gan.layers\n    for epoch in range(n_epochs):\n        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n        for train_batch, _ in dataset.take(n_images // batch_size):\n            # phase 1 - training the discriminator\n            noise = tf.random.normal(shape=[batch_size, codings_size])\n            generated_images = generator(noise)\n            fake_and_real_data = tf.concat([generated_images, train_batch], axis=0)\n            fake_and_real_labels = tf.constant([[0.0]] * batch_size + [[1.0]] * batch_size)\n            discriminator.trainable = True\n            discriminator.train_on_batch(fake_and_real_data, fake_and_real_labels)\n            # phase 2 - training the generator\n            noise = tf.random.normal(shape=[batch_size, codings_size])\n            noise_labels = tf.constant([[1.0]] * batch_size)\n            discriminator.trainable = False\n            gan.train_on_batch(noise, noise_labels)\n        plot_multiple_images(generated_images, 10)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = keras.models.Sequential([generator, discriminator])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator.compile(\n    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop(2e-4)\n)\ndiscriminator.trainable = False\ngan.compile(\n    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop(2e-4)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gan(gan, train_data, N_IMAGES, BATCH_SIZE, codings_size, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}